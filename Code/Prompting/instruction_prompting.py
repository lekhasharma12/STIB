# -*- coding: utf-8 -*-
"""instruction_prompting

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfmqDerMHdH6H95nKIjjiqClensGjox1

**INSTRUCTION PROMPTING**

This file contains code to generate literal translations using Instruction prompting.

You will need to add your OpenAI API key to get the results of prompting. For classsification, the finetuned model is linked to our account. Please ask for the OpenAI API to run the classification if needed.
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install openai
# !pip install datasets
# !pip install git+https://github.com/google-research/bleurt.git
# !pip install transformers
# !pip install bert_score
# !git clone https://github.com/google-research/bleurt.git
# # %cd bleurt
# !pip install .
# # %cd /content/
# !wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .
# !unzip BLEURT-20.zip

# from google.colab import drive
# drive.mount('/content/drive')

import openai
import os
import pandas as pd
import time
import string
from datasets import load_metric, load
from bleurt import score as bleurt_score
from bert_score import score as bert_score

import torch
import random
import numpy as np

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Confirm that the GPU is detected
assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

test_df = pd.read_csv('../Datasets/Prompting/prompting_test.csv')
test_df['Literal Translation'] = ''

classification_key = "<replace_with_your_key>"
prompting_key = "<ask_for_our_model_key>"
openai.api_key = prompting_key

def get_pred(res, labels):
  text = res["choices"][0]["text"].translate(str.maketrans('', '', string.punctuation))
  pred_tokens = text.split(" ")
  pred = "Metaphor"
  for i in pred_tokens:
    if i in labels:
      pred = i
      break
  time.sleep(1)
  return pred.replace("'", "")

def get_type(text):
  openai.api_key = classification_key
  res = openai.Completion.create(model='ada:ft-personal-2023-05-15-01-40-58', prompt=text, temperature=0, top_p=1.0, max_tokens = 100)
  return get_pred(res, ["Sarcasm","Simile","Metaphor","Idiom"])

# call completion model for instruction prompting
def instruction_prompting(df):
  from datasets.tasks import text_classification
  for ind in df.index:
    text = df['Hypothesis'][ind]
    fig_type= get_type(text)
    prompt = "For the given figurative sentence of type " + fig_type + ", provide a literal translation: " + text + "\n"

    print(prompt, end = "")
    lit_translation = openai.Completion.create(
      model="text-davinci-003",
      prompt=prompt, 
      max_tokens = 3000
    )
    print(lit_translation.choices[0].text.strip('\n\n') + '\n\n')
    df.loc[ind, ['Literal Translation']] = lit_translation.choices[0].text.strip('\n\n')
    time.sleep(1)

  print("DONE!")
  return df

test_df = instruction_prompting(test_df)
test_df.shape

test_df

def calculate_bleurt(df):
  # BLEURT calculation
  scorer = bleurt_score.BleurtScorer('/content/BLEURT-20')
  bleurt_scores = scorer.score(references=df['Premise'], candidates=df['Literal Translation'])
  print(bleurt_scores)
  return bleurt_scores

def calculate_bertscore(df):
  # BERTScore calculation
  bertscore = load_metric('bertscore')
  bert_scores = bertscore.compute(predictions=df['Literal Translation'], references=df['Premise'], lang="en")
  print(bert_scores['f1'])
  return bert_scores['f1']

def calculate_littransscore(bert_scores, bleurt_scores):
  lit_trans_scores = []
  for ind in range(len(bleurt_scores)):
    lit_trans_scores.append((bert_scores[ind] + bleurt_scores[ind]) * 50.0)
  return lit_trans_scores

bleurt_scores = calculate_bleurt(test_df)

bert_scores = calculate_bertscore(test_df)

print("Range of BLEURT: ")
print(min(bleurt_scores))
print(max(bleurt_scores), end='\n')

print("Range of BERTScore: ")
print(min(bert_scores))
print(max(bert_scores), end='\n')

# Compute literal translation score
lit_trans_scores = calculate_littransscore(bert_scores, bleurt_scores)
test_df['Score'] = lit_trans_scores
test_df['BLEURT'] = bleurt_scores
test_df['BERT'] = bert_scores

lit_trans_scores_scores_metaphor = test_df.groupby('Type').get_group('Metaphor')['Score']
lit_trans_scores_scores_idiom = test_df.groupby('Type').get_group('Idiom')['Score']
lit_trans_scores_scores_simile = test_df.groupby('Type').get_group('Simile')['Score']
lit_trans_scores_scores_sarcasm = test_df.groupby('Type').get_group('Sarcasm')['Score']

print(min(lit_trans_scores))
print(max(lit_trans_scores))

print('Average literal translation score for metaphor: %d' %(sum(lit_trans_scores_scores_metaphor)/len(lit_trans_scores_scores_metaphor)))
print('Average literal translation score for idiom: %d' %(sum(lit_trans_scores_scores_idiom)/len(lit_trans_scores_scores_idiom)))
print('Average literal translation score for simile: %d' %(sum(lit_trans_scores_scores_simile)/len(lit_trans_scores_scores_simile)))
print('Average literal translation score for sarcasm: %d' %(sum(lit_trans_scores_scores_sarcasm)/len(lit_trans_scores_scores_sarcasm)))

# Export results
test_zero_shot = test_df.loc[:,['Hypothesis','Premise','Literal Translation','Score', 'BLEURT', 'BERT', 'Type','Source']]
test_zero_shot.to_csv('../Results/Prompting/instruction_tuning_new.csv')