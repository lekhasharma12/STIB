# -*- coding: utf-8 -*-
"""zero_shot_prompting

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fGsb-51AM5HnFTt4tSW2T1JHNpjIZVCC

**ZERO SHOT PROMPTING**

This file contains code to generate literal translations zero shot prompting.

You will need to add your OpenAI API key to get the results of prompting.
"""

# Commented out IPython magic to ensure Python compatibility.
# set open_api_key as env variable
# %env OPENAI_API_KEY=<replace_with_your_key>

# Commented out IPython magic to ensure Python compatibility.
# !pip install openai
# !pip install datasets
# !pip install git+https://github.com/google-research/bleurt.git
# !pip install transformers
# !pip install bert_score
# !git clone https://github.com/google-research/bleurt.git
# # %cd bleurt
# !pip install .
# # %cd /content/
# !wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .
# !unzip BLEURT-20.zip

# from google.colab import drive
# drive.mount('/content/drive')

import openai
import os
import pandas as pd
import time
from datasets import load_metric, load
from bleurt import score as bleurt_score
from bert_score import score as bert_score

import torch
import random
import numpy as np

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Confirm that the GPU is detected
assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

test_df = pd.read_csv('../Datasets/Prompting/prompting_test.csv')
test_df['GPT_Translation'] = ''

# set API key
openai.api_key = os.getenv("OPENAI_API_KEY")

# call completion model for zero shot prompting
def zeroshot(fig_input):
  prompt = "Figurative sentence: " + fig_input + "\nLiteral translation:"
  print(prompt, end = "")
  lit_output = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt, 
    max_tokens = 1500
  )
  print(lit_output.choices[0].text.strip('\n') + '\n')
  return lit_output.choices[0].text.strip('\n')

for ind in df.index:
  fig_input = df['Hypothesis'][ind]
  lit_output = zeroshot(fig_input)
  df.loc[ind, ['Literal Translation']] = lit_output
  time.sleep(1)

print("DONE!")

test_df = zeroshot(test_df)
test_df.shape

test_df

def calculate_bleurt(df):
  # BLEURT calculation
  scorer = bleurt_score.BleurtScorer('/content/BLEURT-20')
  bleurt_scores = scorer.score(references=df['Premise'], candidates=df['Literal Translation'])
  print(bleurt_scores)
  return bleurt_scores

def calculate_bertscore(df):
  # BERTScore calculation
  bertscore = load_metric('bertscore')
  bert_scores = bertscore.compute(predictions=df['Literal Translation'], references=df['Premise'], lang="en")
  print(bert_scores['f1'])
  return bert_scores['f1']
  
def calculate_littransscore(bert_scores, bleurt_scores):
  lit_trans_scores = []
  for ind in range(len(bleurt_scores)):
    lit_trans_scores.append((bert_scores[ind] + bleurt_scores[ind]) * 50.0)
  return lit_trans_scores

bleurt_scores = calculate_bleurt(test_df)

bert_scores = calculate_bertscore(test_df)

print("Range of BLEURT: ")
print(min(bleurt_scores))
print(max(bleurt_scores), end='\n')

print("Range of BERTScore: ")
print(min(bert_scores))
print(max(bert_scores), end='\n')

# Compute literal translation score
lit_trans_scores = calculate_littransscore(bert_scores, bleurt_scores)
test_df['Score'] = lit_trans_scores
test_df['BLEURT'] = bleurt_scores
test_df['BERT'] = bert_scores

lit_trans_scores_scores_metaphor = test_df.groupby('Type').get_group('Metaphor')['Score']
lit_trans_scores_scores_idiom = test_df.groupby('Type').get_group('Idiom')['Score']
lit_trans_scores_scores_simile = test_df.groupby('Type').get_group('Simile')['Score']
lit_trans_scores_scores_sarcasm = test_df.groupby('Type').get_group('Sarcasm')['Score']

print(min(lit_trans_scores))
print(max(lit_trans_scores))

print('Average literal translation score for metaphor: %d' %(sum(lit_trans_scores_scores_metaphor)/len(lit_trans_scores_scores_metaphor)))
print('Average literal translation score for idiom: %d' %(sum(lit_trans_scores_scores_idiom)/len(lit_trans_scores_scores_idiom)))
print('Average literal translation score for simile: %d' %(sum(lit_trans_scores_scores_simile)/len(lit_trans_scores_scores_simile)))
print('Average literal translation score for sarcasm: %d' %(sum(lit_trans_scores_scores_sarcasm)/len(lit_trans_scores_scores_sarcasm)))

# Export results
test_zero_shot = test_df.loc[:,['Hypothesis','Literal Translation','Premise','Score','BLEURT','BERT','Type','Source']]
test_zero_shot.to_csv('../Results/Prompting/zero_shot_results_1.csv')