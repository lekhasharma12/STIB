# -*- coding: utf-8 -*-
"""Few-shot-KNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VG3pFjH77GFUKEMLb6gQRo-5vyQGoGX2
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install openai
!pip install datasets
!pip install git+https://github.com/google-research/bleurt.git
!pip install transformers
!pip install bert_score
!git clone https://github.com/google-research/bleurt.git
# %cd bleurt
!pip install .
# %cd /content/
!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .
!unzip BLEURT-20.zip

!pip install openai
!pip install sentence_transformers
!pip install faiss-gpu

import pandas as pd
import sklearn
from sklearn import metrics
import json
import openai
import time
import random
from datasets import load_metric, load
from bleurt import score as bleurt_score
from bert_score import score as bert_score

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

test_df = pd.read_csv('../Datasets/prompting_test.csv')
train_df = pd.read_csv('../Datasets/train-0.3.csv')
premise_train_df = train_df.dropna(subset = ["Premise"])

import torch
import random
import numpy as np

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Confirm that the GPU is detected
assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

prathiksha_api_key = "REPLACE WITH YOUR KEY"
aditya_api_key = "REPLACE WITH YOUR KEY"
openai.api_key = aditya_api_key

def generate_few_shot_prompt(neighbours, df, type_considered=False):
  prompt = ""
  for i in neighbours:
    task_des = ("Give literal translation for this " + df.iloc[i]["Type"] + "\n") if type_considered else ""
    prompt = prompt + task_des + "Figurative sentence: " + df.iloc[i]["Hypothesis"] + "\nLiteral translation: " + df.iloc[i].Premise + "\n\n"
  return prompt

def fewshot_random(input, k, df, type_filtering = False):
  if type_filtering:
    df = premise_train_df[premise_train_df["Type"]==input.Type]
  else :
    df = premise_train_df
  neighbours_indices = random.sample(range(df.shape[0]),k)
  few_shot_prompt = generate_few_shot_prompt(neighbours_indices, df, type_filtering)
  task_des = ("For the given figurative sentences, provide a literal translation: \n\n") if not type_filtering else ""
  test_sample_prompt = ("Give literal translation for this " + get_pred(input.Hypothesis) + "\n") if type_filtering else ""
  test_case = "Figurative sentence: " + input.Hypothesis + "\nLiteral translation: "
  prompt = task_des + few_shot_prompt +test_sample_prompt + test_case
  #print(prompt, end = "")
  lit_output = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt, 
    max_tokens = 1500
  )
  #print(lit_output.choices[0].text.strip('\n') + '\n')
  #print("Actual - ", input.Premise)
  return lit_output.choices[0].text.strip('\n')

df_fewshot_random = test_df.copy()
df_fewshot_random["Literal Translation"] = [""]*df_fewshot_random.shape[0]
for i in range(0,df_fewshot_random.shape[0]):
  input = df_fewshot_random.iloc[i]
  pred = fewshot_random(input,3,False)
  print(pred)
  df_fewshot_random.loc[i,"Literal Translation"] = pred

#check for breaking
list(filter(lambda x:not df_fewshot_random.iloc[x]["Literal Translation"],range(df_fewshot_random.shape[0])))

df_fewshot_random

def calculate_bleurt(df):
  # BLEURT calculation
  scorer = bleurt_score.BleurtScorer('/content/BLEURT-20')
  bleurt_scores = scorer.score(references=df['Premise'], candidates=df['Literal Translation'])
  print(bleurt_scores)
  return bleurt_scores

def calculate_bertscore(df):
  # BERTScore calculation
  bertscore = load_metric('bertscore')
  bert_scores = bertscore.compute(predictions=df['Literal Translation'], references=df['Premise'], lang="en")
  print(bert_scores['f1'])
  return bert_scores['f1']

def calculate_littransscore(bert_scores, bleurt_scores):
  lit_trans_scores = []
  for ind in range(len(bleurt_scores)):
    lit_trans_scores.append((bert_scores[ind] + bleurt_scores[ind]) * 50.0)
  return lit_trans_scores

def calculate_scores(test_df):
  bleurt_scores = calculate_bleurt(test_df)
  bert_scores = calculate_bertscore(test_df)
  
  print("Range of BLEURT: ")
  print(min(bleurt_scores))
  print(max(bleurt_scores), end='\n')

  print("Range of BERTScore: ")
  print(min(bert_scores))
  print(max(bert_scores), end='\n')

  lit_trans_scores = calculate_littransscore(bert_scores, bleurt_scores)
  test_df['Score'] = lit_trans_scores
  test_df['BLEURT'] = bleurt_scores
  test_df['BERT'] = bert_scores

  lit_trans_scores_scores_metaphor = test_df.groupby('Type').get_group('Metaphor')['Score']
  lit_trans_scores_scores_idiom = test_df.groupby('Type').get_group('Idiom')['Score']
  lit_trans_scores_scores_simile = test_df.groupby('Type').get_group('Simile')['Score']
  lit_trans_scores_scores_sarcasm = test_df.groupby('Type').get_group('Sarcasm')['Score']

  print(min(lit_trans_scores))
  print(max(lit_trans_scores))

  print('Average literal translation score for metaphor: %d' %(sum(lit_trans_scores_scores_metaphor)/len(lit_trans_scores_scores_metaphor)))
  print('Average literal translation score for idiom: %d' %(sum(lit_trans_scores_scores_idiom)/len(lit_trans_scores_scores_idiom)))
  print('Average literal translation score for simile: %d' %(sum(lit_trans_scores_scores_simile)/len(lit_trans_scores_scores_simile)))
  print('Average literal translation score for sarcasm: %d' %(sum(lit_trans_scores_scores_sarcasm)/len(lit_trans_scores_scores_sarcasm)))

calculate_scores(df_fewshot_random)

def fewshot_knn(input, k, neighbours,df, type_filtering=True):
  few_shot_prompt = generate_few_shot_prompt(neighbours, df, type_filtering)
  task_des = ("For the given figurative sentences, provide a literal translation: \n\n") if not type_filtering else ""
  test_sample_prompt = ("Give literal translation for this " + get_type(input.Hypothesis) + "\n") if type_filtering else ""
  test_case = "Figurative sentence: " + input.Hypothesis + "\nLiteral translation: "
  prompt = task_des + few_shot_prompt +test_sample_prompt + test_case
  print(prompt, end = "")
  openai.api_key = aditya_api_key
  lit_output = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt, 
    max_tokens = 1500
  )
  return lit_output.choices[0].text.strip('\n')

df_fewshot_random_type_filtering = test_df.copy()
df_fewshot_random_type_filtering["Literal Translation"] = [""]*df_fewshot_random_type_filtering.shape[0]
for i in range(0,df_fewshot_random_type_filtering.shape[0]):
  input = df_fewshot_random.iloc[i]
  pred = fewshot_random(input,3,False)
  print(pred)
  df_fewshot_random.loc[i,"Literal Translation"] = pred

shuffled_train_df = train_df.dropna(subset = ["Premise"]).sample(frac=1)

from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = np.array([model.encode(shuffled_train_df.iloc[0].Hypothesis, convert_to_numpy=True)])
print(embeddings.shape)
for i in range(1,shuffled_train_df.shape[0]):
  sample = shuffled_train_df.iloc[i].Hypothesis
  embeddings = np.append(embeddings,np.array([model.encode(sample, convert_to_numpy=True)]),axis=0)

import faiss
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

def fewshot_knn_embeddings(test_df, type_filtering=False):
  for i in range(test_df.shape[0]):
    input = test_df.iloc[i]
    embed = np.array([model.encode(test_df.iloc[i].Hypothesis, convert_to_numpy=True)])
    D, I = index.search(embed, 3)
    neighbours = I[0]
    literal = fewshot_knn(input,3,neighbours,shuffled_train_df, type_filtering=type_filtering)
    test_df.loc[i,"Literal Translation"] = literal

df_fewshot_knn_without_type = test_df.copy()
df_fewshot_knn_without_type["Literal Translation"] = [""]*df_fewshot_knn_without_type.shape[0]
fewshot_knn_embeddings(df_fewshot_knn_without_type,type_filtering=False)

#check for breaking
list(filter(lambda x:not df_fewshot_knn_without_type.iloc[x]["Literal Translation"],range(df_fewshot_knn_without_type.shape[0])))

df_fewshot_knn_without_type

calculate_scores(df_fewshot_knn_without_type)

df_fewshot_knn_without_type.to_csv("Few-shot-KNN-sampled-from-full-train-without-type.csv")

sarcasm = np.flatnonzero(shuffled_train_df["Type"]=="Sarcasm")
metaphor = np.flatnonzero(shuffled_train_df["Type"]=="Metaphor")
idiom = np.flatnonzero(shuffled_train_df["Type"]=="Idiom")
simile = np.flatnonzero(shuffled_train_df["Type"]=="Simile")

embeddings_sarc = np.array(list(map(lambda x:list(embeddings[x]), sarcasm)))
embeddings_meta = np.array(list(map(lambda x:list(embeddings[x]), metaphor)))
embeddings_idiom = np.array(list(map(lambda x:list(embeddings[x]), idiom)))
embeddings_simile = np.array(list(map(lambda x:list(embeddings[x]), simile)))

index_sarcasm = faiss.IndexFlatL2(embeddings_sarc.shape[1])
index_sarcasm.add(embeddings_sarc)

index_metaphor = faiss.IndexFlatL2(embeddings_meta.shape[1])
index_metaphor.add(embeddings_meta)

index_idiom = faiss.IndexFlatL2(embeddings_idiom.shape[1])
index_idiom.add(embeddings_idiom)

index_simile = faiss.IndexFlatL2(embeddings_simile.shape[1])
index_simile.add(embeddings_simile)

index_mapping = {"Sarcasm":index_sarcasm, "Metaphor":index_metaphor, "Idiom":index_idiom, "Simile":index_simile}

def fewshot_knn_embeddings_with_type_filtering(test_df):
  for i in range(test_df.shape[0]):
    input = test_df.iloc[i]
    embed = np.array([model.encode(test_df.iloc[i].Hypothesis, convert_to_numpy=True)])
    type_index = index_mapping[input.Type]
    D, I = type_index.search(embed, 3)
    neighbours = I[0]
    literal = fewshot_knn(input,3,neighbours,shuffled_train_df)
    test_df.loc[i,"Literal Translation"] = literal

df_fewshot_knn_with_type_filtering = test_df.copy()
df_fewshot_knn_with_type_filtering["Literal Translation"] = [""]*df_fewshot_knn_with_type_filtering.shape[0]
fewshot_knn_embeddings_with_type_filtering(df_fewshot_knn_with_type_filtering)

#check for breaking
list(filter(lambda x:not df_fewshot_knn_with_type_filtering.iloc[x]["Literal Translation"],range(df_fewshot_knn_with_type_filtering.shape[0])))

calculate_scores(df_fewshot_knn_with_type_filtering)

df_fewshot_knn_with_type_filtering.to_csv("Few-shot-KNN-with-type.csv")

df_mapping = {"Sarcasm": sarcasm, "Metaphor": metaphor, "Simile": simile, "Idiom": idiom}

import string
def get_pred(res, labels):
  text = res["choices"][0]["text"].translate(str.maketrans('', '', string.punctuation))
  pred_tokens = text.split(" ")
  pred = "Metaphor"
  for i in pred_tokens:
    if i in labels:
      pred = i
      break
  return pred.replace("'", "")

def get_type(text):
  openai.api_key = prathiksha_api_key
  res = openai.Completion.create(model='ada:ft-personal-2023-05-15-01-40-58', prompt=text, temperature=0, top_p=1.0, max_tokens = 100)
  return get_pred(res, ["Sarcasm","Simile","Metaphor","Idiom"])

#testing which method is the best
test_indices = random.sample(range(test_df.shape[0]),5)
for i in test_indices:
  input = test_df.iloc[i]
  pred_type = get_type(input.Hypothesis)
  pred_random = fewshot_random(input,3,False)
  pred_knn_with_type = fewshot_knn(input,3,random.sample(list(df_mapping[pred_type]),3),shuffled_train_df)
  embed = np.array([model.encode(input.Hypothesis, convert_to_numpy=True)])
  D, I = index.search(embed, 3)
  neighbours = I[0]
  pred_knn_embeddings = fewshot_knn(input,3,neighbours,shuffled_train_df)
  type_index = index_mapping[input.Type]
  D1, I1 = type_index.search(embed, 3)
  neighbours1 = I1[0]
  pred_knn_embeddings_with_type = fewshot_knn(input,3,neighbours1,shuffled_train_df)

  print("Sample: ", input.Hypothesis)
  print("Actual: ", input.Premise)
  print("------------------------------------------------------------------------------------------------------------------------")
  print("Few shot Random: ", pred_random)
  print("Few shot random with type: ", pred_knn_with_type)
  print("Few shot knn: ", pred_knn_embeddings)
  print("Few shot knn with type: ", pred_knn_embeddings_with_type)
  print("\n\n")

random.sample(list(df_mapping["Sarcasm"]),3)

random.sample([1,2,3],3)

