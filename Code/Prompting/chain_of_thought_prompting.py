# -*- coding: utf-8 -*-
"""chain_of_thought_prompting

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2ykzTq54eAE1GyjilRMnwX1JKXFRsmy

**CHAIN OF THOUGHT PROMPTING**

This file contains code to generate literal translations using Chain of Thought (CoT) prompting. We use KNN to get the two most closely resembling samples for our few shot CoT prompt and leverage the explanation for each of these examples as reasoning for the literal translation.


You will need to add your OpenAI API key to get the results of prompting.
For classsification, the finetuned model is linked to our account. Please ask for the OpenAI API to run the classification if needed.

You will need to 
The cells can then be run sequentially once these keys are replaced.
"""

# Commented out IPython magic to ensure Python compatibility.
# compatibility!pip install openai
# !pip install datasets
# !pip install sentence_transformers
# !pip install git+https://github.com/google-research/bleurt.git
# !pip install transformers
# !pip install bert_score
# !git clone https://github.com/google-research/bleurt.git
# # %cd bleurt
# !pip install .
# # %cd /content/
# !wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .
# !unzip BLEURT-20.zip
# !pip install faiss-gpu

# from google.colab import drive
# drive.mount('/content/drive')

import openai
import os
import pandas as pd
import time
import re
from datasets import load_metric, load
from bleurt import score as bleurt_score
from bert_score import score as bert_score
import string
from sentence_transformers import SentenceTransformer, util
import faiss

import torch
import random
import numpy as np

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Confirm that the GPU is detected
assert torch.cuda.is_available()

# Get the GPU device name.
device_name = torch.cuda.get_device_name()
n_gpu = torch.cuda.device_count()
print(f"Found device: {device_name}, n_gpu: {n_gpu}")
device = torch.device("cuda")

test_df = pd.read_csv('../Datasets/Prompting/prompting_test.csv')
test_df["Output"] = ""

train_df = pd.read_csv('../Datasets/Prompting/train-0.3.csv')
# removing datapoints with no explanation
train_df = train_df[train_df.apply(lambda x : pd.isnull(x['Explanation']) != True, axis=1)]

train_df_no_premise = train_df.dropna(subset = ["Premise"])

classification_key = "<replace_with_your_key>"
prompting_key = "<ask_for_our_model_key>"
openai.api_key = prompting_key

def get_type(text):
  openai.api_key = classification_key
  res = openai.Completion.create(model='ada:ft-personal-2023-05-15-01-40-58', prompt=text, temperature=0, top_p=1.0, max_tokens = 100)
  return get_pred(res, ["Sarcasm","Simile","Metaphor","Idiom"])

def get_pred(res, labels):
  text = res["choices"][0]["text"].translate(str.maketrans('', '', string.punctuation))
  pred_tokens = text.split(" ")
  pred = "Metaphor"
  for i in pred_tokens:
    if i in labels:
      pred = i
      break
  return pred.replace("'", "")

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = np.array([model.encode(train_df_no_premise.iloc[0].Hypothesis, convert_to_numpy=True)])
print(embeddings.shape)
for i in range(1,train_df_no_premise.shape[0]):
  sample = train_df_no_premise.iloc[i].Hypothesis
  embeddings = np.append(embeddings,np.array([model.encode(sample, convert_to_numpy=True)]),axis=0)

# select the nearest neighbours from the train dataset for examples in few shot cot
def select_cot_example(train_df, fig_type, fig_input):
  index = faiss.IndexFlatL2(embeddings.shape[1])
  index.add(embeddings)
  embed = np.array([model.encode(fig_input, convert_to_numpy=True)])
  D, I = index.search(embed, 2)
  neighbours = I[0]
  return neighbours

def create_prompt(fig_input, fig_type, neighbours, df):
  prompt = ""
  for ind in neighbours:
    prompt = prompt + "Give literal translation of this " + (df.iloc[ind]['Type']).lower() + ": " + df.iloc[ind]['Hypothesis'] + "\nReasoning: " + df.iloc[ind]['Explanation'] + "\nThis is why the sentence is a " + (df.iloc[ind]['Type']).lower() + ".\nLiteral translation: " + df.iloc[ind]['Premise'] + "\n\n"
  prompt = prompt + "Give literal translation of this " +  fig_type.lower() + ": " + fig_input + "\nReasoning:"
  return prompt

# classify the figurative input and select an example for chain of thought
# generate a prompt using the example and figurative input
# call completion model for CoT prompting

def chain_of_thought(fig_input):
  # replace with classification code
  fig_type = get_type(fig_input)
  
  neighbours = select_cot_example(train_df, fig_type, fig_input)
  
  prompt = create_prompt(fig_input, fig_type, neighbours, train_df)
  print(prompt, end = "")
  
  lit_output = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt, 
    max_tokens = 1500,
  )
  print(lit_output.choices[0].text + '\n')
  return lit_output.choices[0].text

for ind in test_df.index:
    fig_input = test_df['Hypothesis'][ind]
    lit_output = chain_of_thought(fig_input)
    test_df.loc[ind, ['Output']] = lit_output
    time.sleep(1)
    print('-----------')
print("DONE!")

test_df

for ind in test_df.index:
  lit_output = test_df['Output'][ind]
  lit_output = lit_output.strip()
  reasoning = lit_translation = ''
  if re.search("literal translation:", lit_output, re.IGNORECASE):
    reasoning, lit_translation = re.split("literal translation:", lit_output, flags=re.IGNORECASE)
    reasoning = reasoning.strip()
    lit_translation = lit_translation.strip()
  else:
    reasoning = lit_translation = lit_output
  test_df.loc[ind, ['Reasoning']] = reasoning
  test_df.loc[ind, ['Literal Translation']] = lit_translation

test_df

def calculate_bleurt(df):
  # BLEURT calculation
  scorer = bleurt_score.BleurtScorer('/content/BLEURT-20')
  bleurt_scores = scorer.score(references=df['Premise'], candidates=df['Literal Translation'])
  print(bleurt_scores)
  return bleurt_scores

def calculate_bertscore(df):
  # BERTScore calculation
  bertscore = load_metric('bertscore')
  bert_scores = bertscore.compute(predictions=df['Literal Translation'], references=df['Premise'], lang="en")
  print(bert_scores['f1'])
  return bert_scores['f1']

def calculate_littransscore(bert_scores, bleurt_scores):
  lit_trans_scores = []
  for ind in range(len(bleurt_scores)):
    lit_trans_scores.append((bert_scores[ind] + bleurt_scores[ind]) * 50.0)
  return lit_trans_scores

bleurt_scores = calculate_bleurt(test_df)

bert_scores = calculate_bertscore(test_df)

print("Range of BLEURT: ")
print(min(bleurt_scores))
print(max(bleurt_scores), end='\n')

print("Range of BERTScore: ")
print(min(bert_scores))
print(max(bert_scores), end='\n')

# Compute literal translation score
lit_trans_scores = calculate_littransscore(bert_scores, bleurt_scores)
test_df['Score'] = lit_trans_scores
test_df['BLEURT'] = bleurt_scores
test_df['BERT'] = bert_scores

lit_trans_scores_scores_metaphor = test_df.groupby('Type').get_group('Metaphor')['Score']
lit_trans_scores_scores_idiom = test_df.groupby('Type').get_group('Idiom')['Score']
lit_trans_scores_scores_simile = test_df.groupby('Type').get_group('Simile')['Score']
lit_trans_scores_scores_sarcasm = test_df.groupby('Type').get_group('Sarcasm')['Score']

print(min(lit_trans_scores))
print(max(lit_trans_scores))

print('Average literal translation score for metaphor: %d' %(sum(lit_trans_scores_scores_metaphor)/len(lit_trans_scores_scores_metaphor)))
print('Average literal translation score for idiom: %d' %(sum(lit_trans_scores_scores_idiom)/len(lit_trans_scores_scores_idiom)))
print('Average literal translation score for simile: %d' %(sum(lit_trans_scores_scores_simile)/len(lit_trans_scores_scores_simile)))
print('Average literal translation score for sarcasm: %d' %(sum(lit_trans_scores_scores_sarcasm)/len(lit_trans_scores_scores_sarcasm)))

# Export results
test_zero_shot = test_df.loc[:,['Hypothesis','Literal Translation','Premise','Reasoning','Score','BLEURT','BERT','Type','Source']]
test_zero_shot.to_csv('../Results/Prompting/chain_of_thought_results_knn.csv')