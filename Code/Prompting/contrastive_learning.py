# -*- coding: utf-8 -*-
"""685_CP_contrastive_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xOGEWH6Ay49epoEXPsARv20FZfXNZ04B
"""

!pip install sentence-transformers

import pandas as pd
import sklearn
from sklearn import metrics
import json
import time

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

from torch.utils.data import DataLoader
import math
from sentence_transformers import models, losses
from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator,BinaryClassificationEvaluator
import logging
from datetime import datetime
import os
import gzip
import csv
model_name = 'distilbert-base-uncased'

model_save_path = '/content/gdrive/MyDrive/685_CP/Models/CL-'+datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
word_embedding_model = models.Transformer(model_name, max_seq_length=64)

pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

train_df = pd.read_excel('../Datasets/Prompting/train-0.3.xlsx')

from itertools import permutations, combinations
import random
import numpy as np
train_samples = []
hypothesis_list = train_df['Hypothesis'].tolist()
hypothesis_list_train, hypothesis_list_dev, hypothesis_list_test = np.split(hypothesis_list, [int(len(hypothesis_list)*0.95), int(len(hypothesis_list)*0.975)])
#print(hypothesis_list_dev)
hypothesis_list_dev = list(hypothesis_list_dev)
for hypothesis in hypothesis_list_train:
  train_samples.append(InputExample(texts=[hypothesis, hypothesis]))
dev_samples = []
combs = combinations([i for i in range(len(hypothesis_list_dev))],2)
hypothesis_dev_1 = random.sample(hypothesis_list_dev,len(hypothesis_list_dev)//2)
for hypothesis in hypothesis_dev_1:
    score = 1 #Normalize score to range 0 ... 1
    dev_samples.append(InputExample(texts=[hypothesis, hypothesis], label=score))
for comb in combs:
    score = 0
    dev_samples.append(InputExample(texts=[hypothesis_list_dev[comb[0]], hypothesis_list_dev[comb[1]]], label=score))


dev_evaluator = BinaryClassificationEvaluator.from_input_examples(dev_samples, batch_size=8, name='fig-dev')

"""As loss, we use: MultipleNegativesRankingLoss

Here, texts[0] and texts[1] are considered as positive pair, while all others are negatives in a batch
"""

# Configure the training
train_batch_size = 32
num_epochs = 8

# Use MultipleNegativesRankingLoss for SimCSE
train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)
train_loss = losses.MultipleNegativesRankingLoss(model)


warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up
logging.info("Warmup-steps: {}".format(warmup_steps))

logging.info("Performance before training")
print(dev_evaluator(model))

# Train the model
model.fit(train_objectives=[(train_dataloader, train_loss)],
          evaluator=dev_evaluator,
          epochs=num_epochs,
          evaluation_steps=100,
          warmup_steps=warmup_steps,
          output_path=model_save_path
          )

test_samples = []
hypothesis_list_test = list(hypothesis_list_test)
combs = combinations([i for i in range(len(hypothesis_list_test))],2)
hypothesis_test_1 = random.sample(hypothesis_list_test,len(hypothesis_list_test)//2)
for hypothesis in hypothesis_test_1:
    score = 1 #Normalize score to range 0 ... 1
    test_samples.append(InputExample(texts=[hypothesis, hypothesis], label=score))
for comb in combs:
    score = 0
    test_samples.append(InputExample(texts=[hypothesis_list_test[comb[0]], hypothesis_list_test[comb[1]]], label=score))
model = SentenceTransformer(model_save_path)
test_evaluator = BinaryClassificationEvaluator.from_input_examples(test_samples, batch_size=train_batch_size, name='fig-test')
test_evaluator(model, output_path=model_save_path)

model_A = SentenceTransformer('../Models/CL-2023-05-15_08-09-44')

from transformers import AutoModel, AutoTokenizer
import torch

sentence1 = "I was very proud of my sense of judgement when I found out that the person who I admired for their great vision was actually really mean to people"
sentence2 = "I was seriously doubting my sense of judgement when I found out that the person who I admired for their great vision was actually really mean to people"
sentence3 = "It really makes me happy when the person I live with still doesn't want to pay bills on time"

tokenizer = AutoTokenizer.from_pretrained("../Models/CL-2023-05-15_08-09-44")
model = AutoModel.from_pretrained("../Models/CL-2023-05-15_08-09-44")

# Tokenize input texts
texts = [
    sentence1,
    sentence2,
    sentence3
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

# Get the embeddings
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=True, return_dict=True)[1]

embeddings1 = model_A.encode(sentence1)
embeddings2 =  model_A.encode(sentence2)
embeddings3 = model_A.encode(sentence3)

print(embeddings[0].shape)

from scipy.spatial.distance import cosine
print(1-cosine(embeddings1,embeddings2))
print(1-cosine(embeddings1,embeddings3))
print(1-cosine(embeddings2,embeddings3))

model = SentenceTransformer('all-MiniLM-L6-v2')

texts = [
    sentence1,
    sentence2,
    sentence3
]
sentence_embeddings = model.encode(texts)
print(util.cos_sim(sentence_embeddings[0],sentence_embeddings[1]))
print(util.cos_sim(sentence_embeddings[0],sentence_embeddings[2]))
print(util.cos_sim(sentence_embeddings[1],sentence_embeddings[2]))